+++
title = 'Outline : Explaining Llm to Kids'
date = 2025-05-19T20:02:09+13:00
draft = true
tags = ["LLM", "AI", "Kids"]
description = "."
+++

* A simple neural network
* How are these models trained?
* How does all this generate language?

* What makes LLMs work so well?
* Embeddings
* Sub-word tokenizers
* Self-attention
* Softmax
* Residual connections
* Layer Normalization
* Dropout
* Multi-head attention
* Positional embeddings
* The GPT architecture
* The transformer architecture






## Reference 
* [Understanding LLMs from Scratch Using Middle School Math](https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876/)